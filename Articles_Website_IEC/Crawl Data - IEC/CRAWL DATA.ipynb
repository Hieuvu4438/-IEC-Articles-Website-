{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8456da-cbba-4200-8ee3-4a810ce6727d",
   "metadata": {},
   "source": [
    "**DEMO: Crawl Data từ Tech Crunch: Lấy ra các thẻ href là các đường link dẫn tới trang tin tức từ Homepage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b8322e-131b-4c23-8ba1-c5f5f5ae7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "url = \"https://techcrunch.com/latest/\"\n",
    "\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(5)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "driver.quit()\n",
    "articles = soup.find_all(\"a\", class_=\"loop-card__title-link\")\n",
    "links = [a['href'] for a in articles]\n",
    "\n",
    "df = pd.DataFrame(links, columns=[\"Article URL\"])\n",
    "df.to_csv(\"techcrunch_latest_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9e825-d014-46ed-b07e-dae4d482e2a2",
   "metadata": {},
   "source": [
    "**Crawl Data từ 1 trang web nước ngoài + sumamry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f3f1ff-403c-42de-8f7b-a673cb65e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 1000, but your input_length is only 417. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiêu đề: AWS enters into ‘strategic partnership’ with Saudi Arabia-backed Humain\n",
      "URL: https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\n",
      "Ảnh: Không có ảnh\n",
      "\n",
      "📰 Nội dung bài viết đầy đủ:\n",
      "\n",
      "Amazon says it will work with Humain, the AI company recently launched by Saudi Arabia’s ruler, Mohammed bin Salman, to invest “$5 billion-plus” in a strategic partnership to build an “AI Zone” in Saudi Arabia.\n",
      "The AI Zone will include dedicated Amazon Web Services (AWS) AI infrastructure, servers, networks, and training and certification programs, according to a press release. Humain is pledging to develop AI solutions using AWS technologies and to work with AWS on providing access to tools and programs for Saudi Arabia-based AI startups.\n",
      "AWS joins tech giants Nvidia, AMD, and others in partnering with Humain, which is funded by Saudi Arabia’s Public Investment Fund (PIF). American tech firms have looked to the PIF as a source of capital. Companies like Google and Salesforce have also recently worked with the PIF on AI-related projects and investments.\n",
      "President Donald Trump and several tech industry allies attended a U.S.-Saudi investment forum on Tuesday. Under a new Trump administration initiative, U.S.-based tech suppliers, including Nvidia and AMD, have been permitted to arrange deals with Saudi Arabian firms.\n",
      "Saudi Arabia has mandated AI companies and services in the kingdom store data locally, pushing vendors to put facilities there to avoid losing contracts. Both Google and Oracle have announced expansion plans in the region over the past year.\n",
      "Amazon early last March pledged to spend billions of dollars on data centers in Saudi Arabia. On Tuesday, the company said it is devoting around $5.3 billion to develop an AWS region in the kingdom scheduled to come online in 2026. The AI Zone commitments are an “additional investment” separate from the roughly $5.3 billion, Amazon said. It isn’t clear, however, if Amazon’s contribution to the AI Zone will draw on the tranche the company originally announced.\n",
      "TechCrunch has reached out to Amazon for clarification and will update this piece if we hear back.\n",
      "\n",
      "📌 TÓM TẮT:\n",
      "\n",
      "Amazon will invest $5.3 billion in an \"AI Zone\" in Saudi Arabia. The AI Zone will include dedicated Amazon Web Services (AWS) servers, networks, and training and certification programs. The project is funded by Saudi Arabia's Public Investment Fund (PIF)\n",
      "\n",
      "✅ Đã lưu thông tin vào file output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "url = \"https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "title_tag = soup.find('h1', class_='article-hero__title')\n",
    "title = title_tag.text.strip() if title_tag else \"Không tìm thấy tiêu đề\"\n",
    "\n",
    "content_block = soup.find('div', class_='entry-content')\n",
    "paragraphs = content_block.find_all('p') if content_block else []\n",
    "content = '\\n'.join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "images = [img['src'] for img in content_block.find_all('img') if img.get('src')] if content_block else []\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "content_trimmed = content[:3000]\n",
    "summary = summarizer(content, max_length=1000, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(\"Tiêu đề:\", title)\n",
    "print(\"URL:\", url)\n",
    "print(\"Ảnh:\", images if images else \"Không có ảnh\")\n",
    "print(\"\\n📰 Nội dung bài viết đầy đủ:\\n\")\n",
    "print(content)\n",
    "print(\"\\n📌 TÓM TẮT:\\n\")\n",
    "print(summary)\n",
    "\n",
    "with open('output.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['title', 'url', 'summary', 'content', 'images'])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'summary': summary,\n",
    "        'content': content,\n",
    "        'images': ', '.join(images)\n",
    "    })\n",
    "\n",
    "print(\"\\n✅ Đã lưu thông tin vào file output.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44302aa1-5bdd-4656-914b-5058f29fcff8",
   "metadata": {},
   "source": [
    "**DEMO: Crawl Data trên nhiều web khác nhau + summary bằng model HuggingFace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6bd01e-9e32-4f3f-9387-3ad3d4e8ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📰 URL: https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\n",
      "📌 Tiêu đề: AWS enters into ‘strategic partnership’ with Saudi Arabia-backed Humain\n",
      "🖼️ Ảnh: Không có ảnh\n",
      "\n",
      "📄 Nội dung:\n",
      " Amazon says it will work with Humain, the AI company recently launched by Saudi Arabia’s ruler, Mohammed bin Salman, to invest “$5 billion-plus” in a strategic partnership to build an “AI Zone” in Saudi Arabia.\n",
      "The AI Zone will include dedicated Amazon Web Services (AWS) AI infrastructure, servers, networks, and training and certification programs, according to a press release. Humain is pledging to develop AI solutions using AWS technologies and to work with AWS on providing access to tools and programs for Saudi Arabia-based AI startups.\n",
      "AWS joins tech giants Nvidia, AMD, and others in partnering with Humain, which is funded by Saudi Arabia’s Public Investment Fund (PIF). American tech firms have looked to the PIF as a source of capital. Companies like Google and Salesforce have also recently worked with the PIF on AI-related projects and investments.\n",
      "President Donald Trump and several tech industry allies attended a U.S.-Saudi investment forum on Tuesday. Under a new Trump administration initiative, U.S.-based tech suppliers, including Nvidia and AMD, have been permitted to arrange deals with Saudi Arabian firms.\n",
      "Saudi Arabia has mandated AI companies and services in the kingdom store data locally, pushing vendors to put facilities there to avoid losing contracts. Both Google and Oracle have announced expansion plans in the region over the past year.\n",
      "Amazon early last March pledged to spend billions of dollars on data centers in Saudi Arabia. On Tuesday, the company said it is devoting around $5.3 billion to develop an AWS region in the kingdom scheduled to come online in 2026. The AI Zone commitments are an “additional investment” separate from the roughly $5.3 billion, Amazon said. It isn’t clear, however, if Amazon’s contribution to the AI Zone will draw on the tranche the company originally announced.\n",
      "TechCrunch has reached out to Amazon for clarification and will update this piece if we hear back.\n",
      "\n",
      "🧠 Tóm tắt:\n",
      " Amazon will invest $5.3 billion in an \"AI Zone\" in Saudi Arabia. The AI Zone will include dedicated Amazon Web Services (AWS) servers, networks, and training and certification programs. The project is funded by Saudi Arabia's Public Investment Fund (PIF)\n",
      "❌ Lỗi khi tóm tắt bài viết: https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\n",
      "index out of range in self\n",
      "\n",
      "📰 URL: https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\n",
      "📌 Tiêu đề: This new ChatGPT feature solves the most annoying thing about Deep Research\n",
      "🖼️ Ảnh: ['https://cdn.mos.cms.futurecdn.net/omaxTNrWKQyyLuwTVPRAPN.png', 'https://cdn.mos.cms.futurecdn.net/mTaiWitAt8o75BmPY3i4xK.jpg', 'https://cdn.mos.cms.futurecdn.net/8c88QN538yScQuSVce2iym.png', 'https://cdn.mos.cms.futurecdn.net/CnekQ75taQKr2A4BwpNaDQ.jpg', 'https://cdn.mos.cms.futurecdn.net/ujNkHwebXsiadvMMwkynqm.jpg', 'https://cdn.mos.cms.futurecdn.net/ZUqyJvbN2AGcagy7UfZxaj.jpg', 'https://cdn.mos.cms.futurecdn.net/7oCubpx6u5GHwH8Z425yT.jpg', 'https://cdn.mos.cms.futurecdn.net/PWyEJ99KP6Ch6MmR5MdX4J.png', 'https://cdn.mos.cms.futurecdn.net/ViVTMrDq9acYWoJMko5hNR.png', 'https://cdn.mos.cms.futurecdn.net/uYAyjuKucQ4QpzamRdJoeR.jpg', 'https://cdn.mos.cms.futurecdn.net/uhaQk4jWreYwsdkT2AmUuA.jpg', 'https://cdn.mos.cms.futurecdn.net/wVydSQNQJReMzxQNudy2w7.jpg', 'https://cdn.mos.cms.futurecdn.net/StpS7u3AZiQPLgwvLKWn9Y.png', 'https://cdn.mos.cms.futurecdn.net/95neZyuatT65wWMbUv7aJC.jpg', 'https://cdn.mos.cms.futurecdn.net/mJuYV4d7XEFfuXuFWWKshe.jpg', 'https://cdn.mos.cms.futurecdn.net/8kJic4Nst3mXSKgyjMqi6D.jpg', 'https://cdn.mos.cms.futurecdn.net/ViVTMrDq9acYWoJMko5hNR.png', 'https://cdn.mos.cms.futurecdn.net/yhgxFyJ8RbUcoXT7hhSkA7.jpg', 'https://cdn.mos.cms.futurecdn.net/DMdWaFA4DJKzrBHtehFmsN.jpg', 'https://cdn.mos.cms.futurecdn.net/dMHyTqL7epBYHbvgkPkg4h.jpg']\n",
      "\n",
      "📄 Nội dung:\n",
      " I've spent a lot of time experimenting with ChatGPT’s Deep Research feature, and I've produced all kinds of strange (though comprehensive) reports. There's always been a notable gap in its functionality, though, until now. OpenAI has augmented the Deep Research feature with the ability to export your reports as fully formatted PDFs. No more ChatGPT links or screenshots necessary to share what I've learned about the Lake George monster.\n",
      "It's a small interface upgrade, but one that feels like it should have been built into Deep Research from the beginning. Here’s how it works. You make your Deep Research report or pull up one from a while ago, then click on the share icon at the top of the page. You'll see that the usual 'share link' button now has a companion 'download as PDF' button. One click and your report will be a fully formed, citation-rich PDF in your downloads folder.\n",
      "This export option isn't universally available at the moment. You'll need a subscription to ChatGPT Plus, Team, or Pro. Enterprise and Education users don’t have it yet, but OpenAI said it’s coming soon. That's good, as students and professionals are among those I would bet would use Deep Research the most.\n",
      "You can now export your deep research reports as well-formatted PDFs—complete with tables, images, linked citations, and sources.Just click the share icon and select 'Download as PDF.' It works for both new and past reports. pic.twitter.com/kecIR4tEneMay 12, 2025\n",
      "With downloadable PDFs, you can finally do all the things you’d expect to do with your research. That might mean putting it with other research projects, sharing it with teammates, or just attaching it to an email as part of a bet you're going to win.\n",
      "So yes, this is just a PDF button. But it’s a PDF button that fixes what used to be one of ChatGPT’s more frustrating aspects. Now, with downloadable PDFs, you can finally do all the things you’d expect to do with your research: archive it, share it with teammates, attach it to an email, or even – this is my new favorite – upload it to another AI.\n",
      "Yes, really. With the PDF in hand, I popped it into Gemini’s NotebookLM, Google’s own experimental research assistant. Suddenly, the AI was summarizing my Deep Research report, making flashcards, and suggesting related reading. Then I tried uploading the same PDF into a podcast tool and got an AI-generated episode script out of it. Which means, in a roundabout way, ChatGPT just became a content pipeline. One that exports research and lets other tools remix it into whatever format you need.\n",
      "And that’s a huge deal.\n",
      "Sign up for breaking news, reviews, opinion, top tech deals, and more.\n",
      "Because the more AI tools we use, the more we’re going to need bridges between them. OpenAI doesn’t need to be the everything app, but it does need to be interoperable. Giving users a PDF option is low-hanging fruit, sure, but it’s also the kind of fruit that lets you bake an entirely new pie. It makes Deep Research portable. It gives it legs. It means I don’t have to keep 14 tabs open just to reference a well-organized write-up on the history of Japanese vending machines.\n",
      "Of course, OpenAI’s implementation still has quirks. It’s a little confusing that the “Download as PDF” option isn’t in the main chat share menu. Most people will assume it’s not there unless they know where to click. And for a company whose whole pitch is about reducing friction and increasing clarity, burying this behind a second share icon feels oddly off-brand. Still, I’ll take “slightly hidden but fully functional” over “completely missing” any day.\n",
      "More importantly, this change signals something else: OpenAI is listening. Maybe not always quickly. Maybe not always intuitively. But enough people have clearly asked for this (or screamed about it on Reddit) that it finally happened. And in a product landscape where most updates feel like AI models arguing over who’s better at summarizing Aristotle, it’s refreshing to get a feature that solves a real-world problem.\n",
      "Eric Hal Schwartz is a freelance writer for TechRadar with more than 15 years of experience covering the intersection of the world and technology. For the last five years, he served as head writer for Voicebot.ai and was on the leading edge of reporting on generative AI and large language models. He's since become an expert on the products of generative AI models, such as OpenAI’s ChatGPT, Anthropic’s Claude, Google Gemini, and every other synthetic media tool. His experience runs the gamut of media, including print, digital, broadcast, and live events. Now, he's continuing to tell the stories people want and need to hear about the rapidly evolving AI space and its impact on their lives. Eric is based in New York City.\n",
      "Please logout and then login again, you will then be prompted to enter your display name.\n",
      "\n",
      "🧠 Tóm tắt:\n",
      " ChatGPT's Deep Research feature now lets you export your reports as PDFs. It's a small interface upgrade, but one that feels like it should have been built into Deep Research from the beginning. You'll need a subscription to ChatGPT Plus, Team, or Pro to use it yet.\n",
      "\n",
      "✅ Đã lưu vào file articles.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for name, config in PARSERS.items():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    title_tag = soup.select_one(config[\"title\"])\n",
    "    paragraphs = soup.select(config[\"content\"])\n",
    "    images = [img[\"src\"] for img in soup.select(config[\"images\"]) if img.get(\"src\")]\n",
    "\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text.strip())\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"images\": images\n",
    "    }\n",
    "\n",
    "def process_article(url, summarizer):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    domain = urlparse(url).netloc\n",
    "    config = get_parser_by_domain(domain)\n",
    "\n",
    "    if not config:\n",
    "        print(f\"❌ Không có parser cho domain: {domain}\")\n",
    "        return None\n",
    "\n",
    "    article = extract_by_config(soup, config)\n",
    "\n",
    "    if not article[\"title\"] or not article[\"content\"].strip():\n",
    "        print(f\"⚠️ Bỏ qua vì thiếu tiêu đề hoặc nội dung: {url}\")\n",
    "        return None\n",
    "\n",
    "    content_trimmed = article[\"content\"][:3000].strip()\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(content_trimmed, max_length=200, min_length=50, do_sample=False)[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi tóm tắt bài viết: {url}\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n📰 URL:\", url)\n",
    "    print(\"📌 Tiêu đề:\", article[\"title\"])\n",
    "    print(\"🖼️ Ảnh:\", article[\"images\"] if article[\"images\"] else \"Không có ảnh\")\n",
    "    print(\"\\n📄 Nội dung:\\n\", article[\"content\"])\n",
    "    print(\"\\n🧠 Tóm tắt:\\n\", summary)\n",
    "\n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"content\": article[\"content\"],\n",
    "        \"images\": \", \".join(article[\"images\"])\n",
    "    }\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "urls = [\n",
    "    \"https://techcrunch.com/2025/05/13/aws-enters-into-strategic-partnership-with-saudi-arabia-backed-humain/\",\n",
    "    \"https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\",\n",
    "    \"https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for url in urls:\n",
    "    result = process_article(url, summarizer)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "\n",
    "with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\n✅ Đã lưu vào file articles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46034d8-9339-430c-8b15-495a04b05eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parser_config = {\n",
    "    \"techcrunch\": {\n",
    "        \"domain\": \"techcrunch.com\",\n",
    "        \"title\": \"h1.article-hero__title\",\n",
    "        \"content\": \"div.entry-content p\",\n",
    "        \"images\": \"div.entry-content img\",\n",
    "        \"links\": \"\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"title\": \"h1.title-detail\",\n",
    "        \"content\": \"article.fck_detail p\",\n",
    "        \"images\": \"article.fck_detail img\"\n",
    "    },\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"vietnamnet\": {\n",
    "        \"domain\": \"https://vietnamnet.vn\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"genk\": {\n",
    "        \"domain\": \"genk.vn\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\"\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "with open(\"parsers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_config, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be6970-b492-4aec-b659-f6bffa1bd549",
   "metadata": {},
   "source": [
    "**Demo: Crawl Data và summary bằng API Gemini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aad9680d-9fb6-4170-be45-c29440ea0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📰 URL: https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\n",
      "📌 Tiêu đề: Hơn 150 triệu đồng cho các startup tranh tài tại PitchFest 2025\n",
      "🧠 Tóm tắt:\n",
      "Cuộc thi PitchFest, hoạt động trọng tâm của Tuần lễ Blockchain & AI Super Vietnam 2025 (4-5/6 tại Đà Nẵng), tìm kiếm các sáng kiến Web3, blockchain và AI. Tổng giải thưởng tiền mặt 50 triệu đồng chia cho Top 5, kèm gói truyền thông 100 triệu đồng và cơ hội tiếp cận hơn 30 quỹ đầu tư. Cuộc thi gồm 3 vòng: đăng ký, bình chọn (20-27/5 - Top 10 vào chung kết) và chung kết (4/6 - pitching 10 phút). Điều kiện tham gia: có MVP thuộc Web3/AI, tăng trưởng người dùng, đội ngũ kinh nghiệm, vốn gọi dưới 2 triệu USD. Super Vietnam 2025 do Orochi Network, FPT Online và DSAC tổ chức, VnExpress bảo trợ, dự kiến thu hút 7.000 khách, bao gồm hội nghị, triển lãm, kết nối giao thương và các hoạt động khác. Orochi Network, đơn vị phát triển hạ tầng kiểm chứng dữ liệu, có kinh nghiệm trong lĩnh vực an ninh mạng và hợp tác với nhiều dự án Web3.\n",
      "\n",
      "\n",
      "\n",
      "📰 URL: https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\n",
      "📌 Tiêu đề: This new ChatGPT feature solves the most annoying thing about Deep Research\n",
      "🧠 Tóm tắt:\n",
      "ChatGPT Plus, Team, và Pro hiện đã có thêm tính năng mới: xuất báo cáo Deep Research thành file PDF. Trước đây, người dùng chỉ có thể chia sẻ báo cáo qua link hoặc chụp màn hình. Giờ đây, chỉ cần một cú nhấp chuột vào nút \"download as PDF\" trong mục chia sẻ, người dùng có thể tải xuống một file PDF hoàn chỉnh, đầy đủ trích dẫn. Tính năng này giúp người dùng dễ dàng lưu trữ, chia sẻ và sử dụng báo cáo cho nhiều mục đích khác nhau, thậm chí là đưa vào các công cụ AI khác để tạo ra nội dung mới. Mặc dù còn một vài điểm cần cải thiện về giao diện, việc OpenAI lắng nghe và bổ sung tính năng này cho thấy sự quan tâm đến nhu cầu thực tế của người dùng.\n",
      "\n",
      "\n",
      "\n",
      "📰 URL: https://techcrunch.com/2025/05/13/attend-techcrunch-sessions-ai-with-this-new-limited-time-discount/\n",
      "📌 Tiêu đề: Attend TechCrunch Sessions: AI with this new, limited-time discount\n",
      "🧠 Tóm tắt:\n",
      "TechCrunch Sessions: AI giảm giá vé tham dự sự kiện AI tại UC Berkeley vào ngày 5 tháng 6, chỉ còn $292/vé và giảm 50% cho vé thứ hai. Sự kiện này dành cho tất cả mọi người quan tâm đến AI, bao gồm các chuyên gia, nhà sáng lập, học giả, và người đam mê AI. Người tham gia sẽ có cơ hội học hỏi, kết nối, tham gia các buổi networking 1:1 và các sự kiện bên lề do các đối tác tổ chức. Đây là ưu đãi có giới hạn, hãy nhanh tay đăng ký để không bỏ lỡ.\n",
      "\n",
      "\n",
      "\n",
      "✅ Đã lưu xong vào file articles.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "GEMINI_API_KEY = \"AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "\n",
    "def summarize_with_gemini(content):\n",
    "    prompt = (\n",
    "        \"Bạn là một trợ lý AI chuyên tóm tắt tin tức. \"\n",
    "        \"Hãy đọc đoạn nội dung sau và tóm tắt lại đầy đủ, rõ ràng các ý chính\"\n",
    "        \"Hãy viết dễ hiểu, súc tích và giữ đúng tinh thần bài viết.\\n\\n\"\n",
    "        f\"{content.strip()}\"\n",
    "    )\n",
    "\n",
    "    body = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\"text\": prompt}]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    # url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyA6vrOzFkbeSgn0Ixh2EItOOwCxKFsQqgU\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for config in PARSERS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    title_tag = soup.select_one(config[\"title\"])\n",
    "    content_tags = soup.select(config[\"content\"])\n",
    "    image_tags = soup.select(config[\"images\"])\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "    content = \"\\n\".join(p.text.strip() for p in content_tags if p.text.strip())\n",
    "    images = [img[\"src\"] for img in image_tags if img.get(\"src\")]\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"images\": images\n",
    "    }\n",
    "\n",
    "\n",
    "def process_article(url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "\n",
    "    domain = urlparse(url).netloc \n",
    "    config = get_parser_by_domain(domain)\n",
    "\n",
    "    if not config:\n",
    "        print(f\"❌ Không có parser cho domain: {domain}\")\n",
    "        return None\n",
    "\n",
    "    article = extract_by_config(soup, config)\n",
    "    if not article[\"title\"] or not article[\"content\"]:\n",
    "        print(f\"⚠️ Bỏ qua vì thiếu tiêu đề hoặc nội dung: {url}\")\n",
    "        return None\n",
    "\n",
    "    summary = summarize_with_gemini(article[\"content\"][:5000])\n",
    "    if not summary:\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n📰 URL: {url}\")\n",
    "    print(f\"📌 Tiêu đề: {article['title']}\")\n",
    "    # print(f\"Nội dung:{article['content']}\")\n",
    "    print(f\"🧠 Tóm tắt:\\n{summary}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"title\": article[\"title\"],\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"content\": article[\"content\"],\n",
    "        \"images\": \", \".join(article[\"images\"])\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://vnexpress.net/hon-150-trieu-dong-cho-cac-startup-tranh-tai-tai-pitchfest-2025-4885506.html\",\n",
    "        \"https://www.techradar.com/computing/artificial-intelligence/this-new-chatgpt-feature-solves-the-most-annoying-thing-about-deep-research\",\n",
    "        \"https://techcrunch.com/2025/05/13/attend-techcrunch-sessions-ai-with-this-new-limited-time-discount/\"\n",
    "    ]\n",
    "    urls1 = [\"https://techcrunch.com/2025/05/13/anthropic-google-score-win-by-nabbing-openai-backed-harvey-as-a-user/\"]\n",
    "\n",
    "    results = []\n",
    "    for url in urls:\n",
    "        result = process_article(url)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"\\n✅ Đã lưu xong vào file articles.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c6ebd-8559-436a-885b-5cf60edb75af",
   "metadata": {},
   "source": [
    "**Bản nâng cấp CRAWL DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a38a8d-6031-43b1-8d26-8e6733bd451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "parser_config = {\n",
    "    \"techcrunch\": {\n",
    "        \"domain\": \"techcrunch.com\",\n",
    "        \"title\": \"div.article-hero__middle\",\n",
    "        \"content\": \"div.entry-content\",\n",
    "        \"images\": \"div.entry-content img\",\n",
    "        \"author\": \"a.wp-block-tc23-author-card-name__link\",\n",
    "        \"time\": \"time\",\n",
    "        \"highlight\": \"a nofollow\",\n",
    "        \"topic\": \"div.tc23-post-relevant-terms__terms a\",\n",
    "        \"references\": \"div.entry-content a\"\n",
    "    },\n",
    "    \"vnexpress\": {\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"title\": \"h1.title-detail\",\n",
    "        \"content\": \"div.sidebar-1\",\n",
    "        \"images\": \"article.fck_detail img\",\n",
    "        \"author\": \"div.sidebar-1 p.Normal\",\n",
    "        \"time\": \"div.sidebar-1 span.date\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"ul.breadcrumb\",\n",
    "        \"references\": \"div.width_common box-tinlienquanv2\",\n",
    "    },\n",
    "    \"techradar\": {\n",
    "        \"domain\": \"techradar.com\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"references\": \"\",\n",
    "    },\n",
    "    \"vietnamnet\": {\n",
    "        \"domain\": \"https://vietnamnet.vn\",\n",
    "        \"title\": \"div.news-article header h1\",\n",
    "        \"content\": \"div.wcp-item-content p\",\n",
    "        \"images\": \"div.wcp-item-content img\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"highlight\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"references\": \"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"parsers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "parser_links = {\n",
    "    # \"techcrunch\":{\n",
    "    #     \"domain\": \"techcrunch.com\",\n",
    "    #     \"links\": \"a.loop-card__title-link\",\n",
    "    # },\n",
    "    \"vnexpress\":{\n",
    "        \"domain\": \"vnexpress.net\",\n",
    "        \"links\": \"div.wrapper-topstory-folder flexbox width_common wrapper-topstory-folder-v2 no-border a\"\n",
    "    }\n",
    "}\n",
    "with open(\"parsers_links.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parser_links, f, ensure_ascii=False, indent = 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be4f9f-ef3c-42db-8da0-590ee04f4464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Không có parser_links cho domain: techradar.com\n",
      "❌ Không có parser_links cho domain: https://vietnamnet.vn\n",
      "✅ Hoàn thành crawl lúc 2025-07-08 20:24:59\n",
      "❌ Không có parser_links cho domain: techradar.com\n",
      "❌ Không có parser_links cho domain: https://vietnamnet.vn\n",
      "✅ Hoàn thành crawl lúc 2025-07-08 20:27:01\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import schedule\n",
    "\n",
    "with open(\"parsers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS = json.load(f)\n",
    "\n",
    "with open(\"parsers_links.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    PARSERS_LINKS = json.load(f)\n",
    "\n",
    "def get_parser_by_domain(domain):\n",
    "    for config in PARSERS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_parser_link(domain):\n",
    "    for config in PARSERS_LINKS.values():\n",
    "        if config[\"domain\"] in domain:\n",
    "            return config\n",
    "    return None\n",
    "\n",
    "def get_latest_news(soup, config):\n",
    "    links_tags = soup.select(config[\"links\"])\n",
    "    links = [a['href'] for a in links_tags]\n",
    "    return links\n",
    "\n",
    "def extract_by_config(soup, config):\n",
    "    result = {\n",
    "        \"title\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"time\": \"\",\n",
    "        \"topics\": [],\n",
    "        \"content\": []\n",
    "    }\n",
    "    \n",
    "    title_tag = soup.select_one(config.get(\"title\", \"\"))\n",
    "    if title_tag:\n",
    "        result[\"title\"] = title_tag.get_text(strip=True)\n",
    "        \n",
    "    author_tag = soup.select_one(config.get(\"author\", \"\"))\n",
    "    if author_tag:\n",
    "        result[\"author\"] = author_tag.get_text(strip=True)\n",
    "        \n",
    "    time_tag = soup.select_one(config.get(\"time\", \"\"))\n",
    "    if time_tag:\n",
    "        result[\"time\"] = time_tag.get_text(strip=True)\n",
    "        \n",
    "    topic_tags = soup.select(config.get(\"topic\", \"\"))\n",
    "    if topic_tags:\n",
    "        result[\"topics\"] = [tag.get_text(strip=True) for tag in topic_tags]\n",
    "        \n",
    "    content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "    if content_container:\n",
    "        all_elements = content_container.find_all(recursive=True)\n",
    "        for element in all_elements:\n",
    "            if element.name == \"p\" and element.get_text(strip=True):\n",
    "                result[\"content\"].append({\"type\": \"text\", \"value\": element.get_text(strip=True)})\n",
    "            elif element.name == \"img\" and element.get(\"src\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"image\", \"value\": element.get(\"src\")})\n",
    "            elif element.name == \"a\" and element.get(\"href\", \"\"):\n",
    "                result[\"content\"].append({\"type\": \"link\", \"value\": element.get(\"href\")})\n",
    "                \n",
    "    return result\n",
    "\n",
    "def summarize(soup, config):\n",
    "    try:\n",
    "        title_tag = soup.select_one(config[\"title\"])\n",
    "        content_tags = soup.select(config[\"content\"])\n",
    "        image_tags = soup.select(config[\"images\"])\n",
    "        author_tag = soup.select_one(config[\"author\"])\n",
    "        time_tag = soup.select_one(config[\"time\"])\n",
    "        topics_tag = soup.select(config[\"topic\"])\n",
    "        references_tags = soup.select(config[\"references\"])\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        content = \"\\n\".join(p.text.strip() for p in content_tags if p.text.strip())\n",
    "        images = [img[\"src\"] for img in image_tags if img.get(\"src\")]\n",
    "        author = author_tag.text.strip() if author_tag else None\n",
    "        time = time_tag.text.strip() if time_tag else None\n",
    "        topics = [topic.get_text(strip=True) for topic in topics_tag]\n",
    "        references = href_list = [link['href'] for link in references_tags if link.get('href')]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"content\": content,\n",
    "            \"images\": images,\n",
    "            \"author\": author,\n",
    "            \"time\": time,\n",
    "            \"topic\": topics,\n",
    "            \"references\": references,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi trích xuất dữ liệu: {e}\")\n",
    "        return None\n",
    "\n",
    "def parser_seo(soup, config):\n",
    "    try:\n",
    "        seo_data = {\n",
    "            \"meta_title\": \"\",\n",
    "            \"meta_description\": \"\",\n",
    "            \"meta_keywords\": [],\n",
    "            \"h1\": \"\",\n",
    "            \"h2\": [],\n",
    "            \"canonical_url\": \"\",\n",
    "            \"word_count\": 0,\n",
    "            \"internal_links\": [],\n",
    "            \"external_links\": []\n",
    "        }\n",
    "        meta_title = soup.find(\"meta\", property=\"og:title\") or soup.find(\"meta\", attrs={\"name\": \"title\"})\n",
    "        if meta_title and meta_title.get(\"content\"):\n",
    "            seo_data[\"meta_title\"] = meta_title[\"content\"]\n",
    "        meta_desc = soup.find(\"meta\", property=\"og:description\") or soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        if meta_desc and meta_desc.get(\"content\"):\n",
    "            seo_data[\"meta_description\"] = meta_desc[\"content\"]\n",
    "        meta_keywords = soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "        if meta_keywords and meta_keywords.get(\"content\"):\n",
    "            seo_data[\"meta_keywords\"] = [kw.strip() for kw in meta_keywords[\"content\"].split(\",\")]\n",
    "        h1_tag = soup.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            seo_data[\"h1\"] = h1_tag.get_text(strip=True)\n",
    "        h2_tags = soup.find_all(\"h2\")\n",
    "        if h2_tags:\n",
    "            seo_data[\"h2\"] = [h2.get_text(strip=True) for h2 in h2_tags]\n",
    "        canonical = soup.find(\"link\", rel=\"canonical\")\n",
    "        if canonical and canonical.get(\"href\"):\n",
    "            seo_data[\"canonical_url\"] = canonical[\"href\"]\n",
    "        content_container = soup.select_one(config.get(\"content\", \"\"))\n",
    "        if content_container:\n",
    "            text_content = \" \".join(p.get_text(strip=True) for p in content_container.find_all(\"p\"))\n",
    "            seo_data[\"word_count\"] = len(text_content.split())\n",
    "\n",
    "        domain = config[\"domain\"]\n",
    "        links = soup.select(config.get(\"references\", \"\"))\n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                if domain in href or href.startswith(\"/\"):\n",
    "                    seo_data[\"internal_links\"].append(href)\n",
    "                else:\n",
    "                    seo_data[\"external_links\"].append(href)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi phân tích SEO: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_seo_inf(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc\n",
    "        config = get_parser_by_domain(domain)\n",
    "\n",
    "        if not config:\n",
    "            print(f\"❌ Không có parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        if not seo_data:\n",
    "            print(f\"⚠️ Không thể thu thập dữ liệu SEO cho: {url}\")\n",
    "            return None\n",
    "        output_file = \"seo_data.json\"\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_data = []\n",
    "\n",
    "        existing_data.append({\n",
    "            \"url\": url,\n",
    "            \"domain\": domain,\n",
    "            \"seo_data\": seo_data\n",
    "        })\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        return seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi xử lý URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_article(url):\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        driver.quit()\n",
    "        domain = urlparse(url).netloc \n",
    "        config = get_parser_by_domain(domain)\n",
    "        config_links = get_parser_link(domain)\n",
    "        \n",
    "        if not config:\n",
    "            print(f\"❌ Không có parser cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        if not config_links:\n",
    "            print(f\"❌ Không có parser_links cho domain: {domain}\")\n",
    "            return None\n",
    "\n",
    "        links = get_latest_news(soup, config_links)\n",
    "        article = extract_by_config(soup, config)\n",
    "        summarize_article = summarize(soup, config)\n",
    "        seo_data = parser_seo(soup, config)\n",
    "        \n",
    "        if not article or not article[\"title\"] or not article[\"content\"]:\n",
    "            print(f\"⚠️ Bỏ qua vì thiếu tiêu đề hoặc nội dung: {url}\")\n",
    "            return None\n",
    "            \n",
    "        return article, summarize_article, seo_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi xử lý URL {url}: {e}\")   \n",
    "        return None\n",
    "\n",
    "def crawl_multiple_urls():\n",
    "    try:\n",
    "        articles = []\n",
    "        summaries = []\n",
    "        seo_data_list = []\n",
    "        domains = [config[\"domain\"] for config in PARSERS.values()]\n",
    "        for domain in domains:\n",
    "            config_links = get_parser_link(domain)\n",
    "            if not config_links:\n",
    "                print(f\"❌ Không có parser_links cho domain: {domain}\")\n",
    "                continue\n",
    "            options = Options()\n",
    "            options.headless = True\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            driver.get(f\"https://{domain}\")\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            driver.quit()\n",
    "            links = get_latest_news(soup, config_links)\n",
    "            for url in links[:2]:\n",
    "                if url.startswith(\"/\"):\n",
    "                    url = f\"https://{domain}{url}\"\n",
    "\n",
    "                result = process_article(url)\n",
    "                if result:\n",
    "                    article, summarize_article, seo_data = result\n",
    "                    articles.append(article)\n",
    "                    summaries.append(summarize_article)\n",
    "                    seo_data_list.append({\n",
    "                        \"url\": url,\n",
    "                        \"domain\": domain,\n",
    "                        \"seo_data\": seo_data\n",
    "                    })\n",
    "        with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"articles_summarize.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summaries, f, ensure_ascii=False, indent=4)\n",
    "        with open(\"seo_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(seo_data_list, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"✅ Hoàn thành crawl lúc {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Lỗi khi crawl multiple URLs: {e}\")\n",
    "\n",
    "def start_crawling():\n",
    "    schedule.every(1).minutes.do(crawl_multiple_urls)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_crawling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
